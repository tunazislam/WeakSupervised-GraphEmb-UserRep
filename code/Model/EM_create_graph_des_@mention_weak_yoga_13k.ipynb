{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "from numpy import array\n",
    "import re\n",
    "import pickle\n",
    "#from pattern.text.en import singularize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "#import enchant\n",
    "from nltk.stem import LancasterStemmer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "import operator\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "\n",
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['hundred', 'thousand', 'news', 'daily', 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'per', 'a', 'an',\\\n",
    "                   'the', 'and', 'but', 'or', 'should', 'would', 'might', 'could', 'of', 'with', 'at', 'from', 'into', 'during', 'including', 'until', 'against',\\\n",
    "                   'among', 'throughout', 'despite', 'towards', 'upon', 'concerning', 'to', 'in', 'for', 'on', 'by', 'about', 'like', 'through', 'over', 'before',\\\n",
    "                   'between', 'after', 'since', 'without', 'under', 'within', 'along', 'following', 'across', 'behind', 'beyond', 'plus', 'except', 'but', 'up',\\\n",
    "                   'out', 'around', 'down', 'off', 'above', 'near', 'r',\\\n",
    "                    'today','tomorrow','yesterday','todays','everyday','m','s','re']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utypes = {'utype_prac' : 0, 'utype_promo' : 1, 'utype_none' : -1}\n",
    "reverse_utypes = {'0' : 'utype_prac', '1' : 'utype_promo', '-1' : 'utype_none'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get stop words of all languages\n",
    "STOPWORDS_DICT = {lang: set(nltk.corpus.stopwords.words(lang)) for lang in nltk.corpus.stopwords.fileids()}\n",
    "#get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "#load spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean and lemmatize ads\n",
    "def clean_ads(text):\n",
    "    #print(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #delete http\n",
    "    text = re.sub(r\"www\\S+\", \"\", text)\n",
    "    text = re.sub(\"\\d\", \"\", text) #remove digit\n",
    "    text = re.sub(r'[^\\w\\d\\s]+', '', text) #remove punctuations\n",
    "    #print(text)\n",
    "    #remove punctuations\n",
    "#     regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')   \n",
    "#     nopunct = regex.sub(\" \", str(text))\n",
    "    #print(nopunct)\n",
    "    \n",
    "    #use spacy to lemmatize comments\n",
    "    #doc = nlp(nopunct, disable=['parser','ner'])\n",
    "    doc = nlp(text, disable=['parser','ner'])\n",
    "    #lemma = [token.lemma_ for token in doc]\n",
    "    lemma = [token.lemma_.lower() if token.lemma_ != '-PRON-' else token.lower_ for token in doc]  #remove '-PRON-'\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word in stop_words:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rightTypesTri(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or '  ' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word in stop_words:\n",
    "            return False\n",
    "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ngrams(unlist_ads):\n",
    "    ##Initialize NLTK's Bigrams/Trigrams Finder\n",
    "    bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "    trigrams = nltk.collocations.TrigramAssocMeasures()\n",
    "    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_ads)\n",
    "    trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_ads)\n",
    "\n",
    "    ##frequency based\n",
    "    ##Counting Frequencies of Adjacent Words filter for only adjectives and nouns\n",
    "    bigram_freq = bigramFinder.ngram_fd.items()\n",
    "    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "    #filter bigrams\n",
    "    filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "    #print(\"filtered_bi\", len(filtered_bi))\n",
    "    ## trigram\n",
    "    trigram_freq = trigramFinder.ngram_fd.items()\n",
    "    trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
    "    #filter trigrams\n",
    "    filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n",
    "    #print(\"filtered_tri\", len(filtered_bi), len(filtered_tri))\n",
    "    #print(filtered_tri)\n",
    "    ##if we choose freq >= 2\n",
    "    #new_filtered_tri = filtered_tri.loc[filtered_tri['freq'] >= 2]\n",
    "    #print(new_filtered_tri)\n",
    "    #print(\"filtered_tri\", len(filtered_tri), len(new_filtered_tri))\n",
    "    freq_bi= filtered_bi[:].bigram.values \n",
    "    freq_tri = filtered_tri[:].trigram.values \n",
    "    \n",
    "    ##convert comma separated trigram list to \"_\" separated trigram\n",
    "    #print(str(freq_tri[0]).replace(\"'\",\"\").replace(\" \",\"\").replace(\",\",\"_\").replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "    new_freq_bi = [str(trig).replace(\"'\",\"\").replace(\" \",\"\").replace(\",\",\"_\").replace(\"(\",\"\").replace(\")\",\"\") for trig in [trigs for trigs in freq_bi]]\n",
    "    new_freq_tri = [str(trig).replace(\"'\",\"\").replace(\" \",\"\").replace(\",\",\"_\").replace(\"(\",\"\").replace(\")\",\"\") for trig in [trigs for trigs in freq_tri]]\n",
    "    \n",
    "    #print(new_freq_tri, len(new_freq_tri), type(new_freq_tri))\n",
    "    return new_freq_bi, new_freq_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_CSV():\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    name2text={} #user_name -> tweets\n",
    "    name2loc={} #user_name -> location\n",
    "    name2des={} #user_name -> description\n",
    "    name2utype={} ##user_name -> utype\n",
    "    name2utype_gt = {} ##user_name -> utype_gt\n",
    "    des2utype={} #description -> utype \n",
    "    loc2utype={} #location -> utype \n",
    "\n",
    "    row_count=-1\n",
    "    \n",
    "        \n",
    "    #with open(\"data/weakly_all_des_gt_mergetweets_yoga_13k_em1_des_net.csv\", \"r\") as csv_file: #for em1\n",
    "    with open(\"data/weakly_all_des_gt_mergetweets_yoga_13k_em2_des_net.csv\", \"r\") as csv_file: #for em2\n",
    "    #with open(\"data/weakly_all_des_gt_mergetweets_yoga_13k_em3_des_net.csv\", \"r\") as csv_file: #for em3\n",
    "    #with open(\"data/weakly_all_des_gt_mergetweets_yoga_13k_em4_des_net.csv\", \"r\") as csv_file: #for em4\n",
    "    \n",
    "        csv_reader = csv.reader(csv_file, delimiter=str(','), quotechar=str('\"'))\n",
    "        for row in csv_reader:\n",
    "            # print (row_count) #upto 13k ads\n",
    "            #print(row)\n",
    "            row_count += 1\n",
    "            if row_count==0:\n",
    "                continue\n",
    "\n",
    "            name=str(row[0])\n",
    "            loc = str(row[1]).lower()\n",
    "            #print('loc', len(loc))\n",
    "            des=str(row[2]).lower()\n",
    "            #print('des', len(des))\n",
    "            #utype = str(row[4]) #before em1\n",
    "            #utype = str(row[6]) #for em1\n",
    "            utype = str(row[7]) #for em2\n",
    "            #utype = str(row[8]) #for em3\n",
    "            #utype = str(row[9]) #for em4\n",
    "            #print('utype', utype, type(utype))\n",
    "            utype_gt = str(row[5]) #gold\n",
    "            twt=str(row[3])\n",
    "            text=twt\n",
    "\n",
    "            #print(text)\n",
    "            \n",
    "            \n",
    "            lemmatized = clean_ads(str(text))\n",
    "            #print(lemmatized)\n",
    "            # #turn all tokens into one single list\n",
    "            unlist_ads = [item for item in lemmatized]\n",
    "            # using join() +  split() to \n",
    "            # perform removal of empty string\n",
    "            unlist_ads = ' '.join(unlist_ads).split() \n",
    "            #print(unlist_ads)\n",
    "            if unlist_ads and len(unlist_ads) >= 1 : #if twt length is >= 1\n",
    "                \n",
    "                #new_freq_bi, new_freq_tri = get_ngrams(unlist_ads)\n",
    "                #print(new_freq_bi)\n",
    "#                 utype = str (row[4])\n",
    "#                 utype_gt = str(row[5]) #gold\n",
    "\n",
    "                name2text[name]=unlist_ads ##user_name -> tweets\n",
    "                name2loc[name]= loc #user_name -> location\n",
    "                name2des[name]= des #user_name -> description\n",
    "                name2utype[name]= utype.split('.')[0] #utype##user_name -> utype\n",
    "                name2utype_gt[name] = utype_gt.split('.')[0] ##user_name -> utype_gt\n",
    "            if utype:\n",
    "                des2utype[des] = utype.split('.')[0]#int(float(utype))  #description -> utype \n",
    "                loc2utype[loc] = utype.split('.')[0] #int(float(utype)) #location -> utype \n",
    "#             if des in des2utype:\n",
    "#                 print (des, utype) \n",
    "#             des2utype[des] = utype\n",
    "\n",
    "\n",
    "    print (len(name2text), len(des2utype), len(loc2utype)) #13301 13178 7302\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    with open('data/graph_embd_yoga/processed_data_des_net_em2.pickle', 'wb') as out_file:\n",
    "        pickle.dump([name2text, name2loc, name2des, name2utype, name2utype_gt, des2utype, loc2utype], out_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read_CSV() ###create pickle file for processed data (13k ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove={}\n",
    "def ProcessGlove():\n",
    "    global glove\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(\"data/glove.6B.300d.txt\", 'r')\n",
    "    #f = open(\"../../Community_Detection/Resources/glove.twitter.27B/glove.twitter.27B.200d.txt\", 'r')\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        glove[word] = embedding\n",
    "    print (\"Done.\", len(glove), \" words loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize_sentences(id2text, name2id):\n",
    "    ad2tokenized_text={}\n",
    "    idx2word={}\n",
    "    word2idx={}\n",
    "    word2count={}\n",
    "\n",
    "    word2idx['<pad>'] = len(word2idx)\n",
    "    idx2word[len(idx2word)] = '<pad>'\n",
    "    for segment_id in id2text:\n",
    "        tokenized_text=[]\n",
    "        text = id2text[segment_id]\n",
    "        #print(text)\n",
    "#         text = text.lower()\n",
    "#         text = text.split()\n",
    "        for w in text:\n",
    "\n",
    "            '''\n",
    "            if w in stopwords.words('english'):\n",
    "                continue\n",
    "            '''\n",
    "            #if w!=\"\" and w not in word2idx and is_ascii(w) and w.isdigit() == False:\n",
    "            if w!=\"\" and w not in word2idx and w.isdigit() == False:\n",
    "                word2idx[w]=len(word2idx)\n",
    "                idx2word[len(idx2word)]=w\n",
    "\n",
    "            '''\n",
    "            if w!=\"\" and is_ascii(w):\n",
    "                tokenized_text.append(w)\n",
    "            '''\n",
    "            if w in word2idx:\n",
    "                tokenized_text.append(word2idx[w])\n",
    "            #if w!=\"\" and is_ascii(w) and w.isdigit() == False and w in word2count:\n",
    "            if w!=\"\" and w.isdigit() == False and w in word2count:\n",
    "                word2count[w]=word2count[w]+1\n",
    "            #elif w!=\"\" and is_ascii(w) and w.isdigit() == False and w not in word2count:\n",
    "            elif w!=\"\" and w.isdigit() == False and w not in word2count:\n",
    "                word2count[w]=1\n",
    "        ad2tokenized_text[name2id[segment_id]]=tokenized_text\n",
    "        #print tokenized_text\n",
    "        if len(tokenized_text)==0:\n",
    "            print (text)\n",
    "\n",
    "    print (\"Tokenization complete!\")\n",
    "    ProcessGlove()\n",
    "    #glove = torchtext.vocab.GloVe(name=\\\"6B\\\",dim=300)   # embedding size = 300\n",
    "#     glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\\n\"\n",
    "#                                   dim=300)   # embedding size = 300\"\n",
    "   \n",
    "#     print('first', glove[idx2word[0]])  \n",
    "#     print('second', glove[idx2word[1]])\n",
    "    #sys.exit()\n",
    "    matrix_len = len(idx2word)\n",
    "    weights_matrix = np.zeros((matrix_len, 300))\n",
    "    words_found = 0\n",
    "    not_found=[]\n",
    "    for idx in idx2word:\n",
    "#         print(\"idx2word\", len(idx2word)) #44386\n",
    "#         print(\"idx\", idx) #idx 0\n",
    "#         print(\"idx2word[idx]\", idx2word[0], idx2word[1]) ### <pad> , trump\n",
    "        try:\n",
    "            weights_matrix[idx] = glove[idx2word[idx]]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            not_found.append(idx2word[idx])\n",
    "            if idx==0:\n",
    "                weights_matrix[idx] = np.random.normal(scale=0, size=(300,))\n",
    "            else:\n",
    "                weights_matrix[idx] = np.random.normal(scale=1/float(300), size=(300,))\n",
    "\n",
    "    print (\"not found: \", len(not_found), \"found: \", words_found) #not found:  218162 found:  47851\n",
    "    print (not_found[:10])\n",
    "\n",
    "    single_count=0\n",
    "    for w in not_found:\n",
    "        if w=='<pad>':\n",
    "            continue\n",
    "        if word2count[w]==1:\n",
    "            single_count+=1\n",
    "    print (\"single_count\", single_count) #153485 #162228\n",
    "\n",
    "    return ad2tokenized_text, weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Graph():\n",
    "\n",
    "    with open('data/graph_embd_yoga/processed_data_des_net_em2.pickle', 'rb') as in_file:\n",
    "        [name2text, name2loc, name2des, name2utype, name2utype_gt, des2utype, loc2utype]=pickle.load(in_file)\n",
    "        \n",
    "    with open('data/graph_embd_yoga/mention_yoga.pickle', 'rb') as in_file:\n",
    "        [name2mention]=pickle.load(in_file)\n",
    "\n",
    "\n",
    "#     name2text['prac']='yoga fan'\n",
    "#     name2text['prac']='yoga enthusiast'\n",
    "#     name2text['promo']='studio that shape your life'\n",
    "#     name2text['promo']='book your free class'\n",
    "\n",
    "\n",
    "\n",
    "    all_types=[typ for typ in utypes]\n",
    "    #print(all_types, type(all_types)) #['type_prac', 'type_promo', 'type_none'] <class 'list'>\n",
    "    all_users=[name for name in name2loc] \n",
    "    #print(all_users, type(all_users)) #['000813_000606', '000mrs000', '001bhaskarkumar'..], list\n",
    "    all_descriptions=list(set([name2des[name] for name in name2des]))\n",
    "    print(\"all_descriptions\", type(all_descriptions), len(all_descriptions), len(name2des))\n",
    "    all_locations=list(set([name2loc[name] for name in name2loc]))\n",
    "    print('all_locations', len(all_locations), len(name2loc))\n",
    "\n",
    "    \n",
    "    name2id={}\n",
    "    id2name={}\n",
    "    id=0\n",
    "    type_start=id\n",
    "    for typ in all_types:\n",
    "        name2id[typ]=id\n",
    "        id2name[id]=typ\n",
    "        id+=1\n",
    "    type_end=id-1\n",
    "\n",
    "#     id=10\n",
    "#     indicator_label_start = id\n",
    "#     for il in all_indicator_labels:\n",
    "#         name2id[il] = id\n",
    "#         id2name[id] = il\n",
    "#         id += 1\n",
    "#     indicator_label_end = id - 1\n",
    "\n",
    "#     id=20\n",
    "#     issue_start = id\n",
    "#     for issue in all_issues:\n",
    "#         name2id[issue] = id\n",
    "#         id2name[id] = issue\n",
    "#         id += 1\n",
    "#     issue_end = id - 1\n",
    "\n",
    "    id=100000\n",
    "    user_start = id\n",
    "    for user in all_users:\n",
    "        name2id[user] = id\n",
    "        id2name[id] = user\n",
    "        id += 1\n",
    "    user_end = id - 1\n",
    "\n",
    "    id=1000000\n",
    "    des_start = id\n",
    "    for des in all_descriptions:\n",
    "        name2id[des] = id\n",
    "        id2name[id] = des\n",
    "        id += 1\n",
    "    des_end = id - 1\n",
    "    \n",
    "    id=2000000\n",
    "    loc_start = id\n",
    "    for loc in all_locations:\n",
    "        name2id[loc] = id\n",
    "        id2name[id] = loc\n",
    "        id += 1\n",
    "    loc_end = id - 1\n",
    "    \n",
    "\n",
    "    graph={}\n",
    "\n",
    "    for id in id2name:\n",
    "        graph[id]=set()\n",
    "    #print(graph) #{0: set(), 1: set(), ......}\n",
    "    \n",
    "\n",
    "    '''\n",
    "    user to description adjacency\n",
    "    '''\n",
    "    for user in all_users:\n",
    "        if user not in name2des:\n",
    "            continue\n",
    "        des = [name2des[user]]\n",
    "        des = [name2id[d] for d in des]\n",
    "\n",
    "        graph[name2id[user]] = graph[name2id[user]] | set(des)\n",
    "    #print(graph)  \n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    description to utype adjacency\n",
    "    '''\n",
    "    annotated_des=[]\n",
    "    for des in all_descriptions:\n",
    "        if des in des2utype:\n",
    "#             print('des', des)\n",
    "#             #print(des2utype)\n",
    "#             print('des2utype[des]', des2utype[des], type(des2utype[des]))\n",
    "#             print('reverse_utypes[des2utype[des]]', reverse_utypes[des2utype[des]])\n",
    "            types=name2id[reverse_utypes[des2utype[des]]]\n",
    "            graph[name2id[des]]=graph[name2id[des]] | set([types])\n",
    "            annotated_des.append(name2id[des])\n",
    "    #print(annotated_des, len(annotated_des))\n",
    "    #print(graph) #{...1000191: {1}, 1000192: set()...}\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    user to utype adjacency\n",
    "    '''\n",
    "    annotated_user=[]\n",
    "    for id in name2utype:\n",
    "        #print(id)\n",
    "        #print(name2utype[id], type(name2utype[id]), len(name2utype[id]))\n",
    "        for ty in name2utype[id]:\n",
    "            if ty:\n",
    "                #print('hi')\n",
    "                #print(reverse_stances[st], type(reverse_stances[st])) #stance_anti-trump \n",
    "                utype=name2id[reverse_utypes[ty]] \n",
    "                #print(stance) #3\n",
    "                graph[name2id[id]]=graph[name2id[id]] | set([utype])\n",
    "                annotated_user.append(name2id[id])\n",
    "        #graph[name2id[id]]=graph[name2id[id]] | set([name2id[st] for st in id2inferred_stance[id]])\n",
    "    #print(graph) # {...100004: {2000826, 3, 1000018, 2001366}, 100005: {2000472, 0, 1000018, 3},....}\n",
    "    \n",
    "    '''\n",
    "    user to @mention user from our data\n",
    "    '''\n",
    "    \n",
    "    for id in name2utype:\n",
    "        #print('id', id)\n",
    "        \n",
    "        #print('name2id[id]', name2id[id])\n",
    "        for um in name2mention[id]:\n",
    "            #print('ty', ty)\n",
    "            if um:\n",
    "                #print('hi')\n",
    "                \n",
    "                mention=name2id[um] \n",
    "                \n",
    "                graph[name2id[id]]=graph[name2id[id]] | set([mention])\n",
    "    #print(graph)         \n",
    "                \n",
    "    #////uncomment\n",
    "    \n",
    "    f1=open('data/graph_embd_yoga/yoga_graph_des_net_em2.mapping', 'w')\n",
    "    f2=open('data/graph_embd_yoga/yoga_graph_des_net_em2.adjlist', 'w')\n",
    "\n",
    "    all_ids=sorted(id for id in id2name)\n",
    "\n",
    "    for id in all_ids:\n",
    "        f1.write(str(id)+\"\\t\"+str(id2name[id])+\"\\n\")\n",
    "        f2.write(str(id)+\"\\t\")\n",
    "        adjlist=sorted(list(graph[id]))\n",
    "\n",
    "        for adj in range(len(adjlist)):\n",
    "            if adj==0:\n",
    "                f2.write(str(adjlist[adj]))\n",
    "            else:\n",
    "                f2.write(\" \"+str(adjlist[adj]))\n",
    "        f2.write(\"\\n\")\n",
    "\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "        #print(id2text)\n",
    "    \n",
    "        \n",
    "    twt2tokenized_text, weights_matrix=Tokenize_sentences(name2text, name2id)\n",
    "\n",
    "    '''\n",
    "     Create validation set using our manually annotated ground truth\n",
    "    '''\n",
    "    #ad2annotated_stance={}\n",
    "    gt_user = {}\n",
    "    #print(\"name2utype_gt\", name2utype_gt)\n",
    "    for id in name2utype_gt:\n",
    "        #print(id)\n",
    "        #print('name2utype_gt[id]',name2utype_gt[id], type(name2utype_gt[id]))\n",
    "        #if id2stance[id][0] != '' and id not in annotated_ads and int(id2stance[id][0])!=-1:\n",
    "        #if id2stance[id][0] != '' and int(id2stance[id][0]) != -1:\n",
    "        if name2utype_gt[id] != '' and name2utype_gt[id] != '2':\n",
    "            #gt_utypes=[name2id[reverse_utypes[gt]] for gt in name2utype_gt[id]]\n",
    "            gt_utypes= name2id[reverse_utypes[name2utype_gt[id]]]\n",
    "            #print('gt_utypes', gt_utypes, type(gt_utypes))\n",
    "            gt_user[name2id[id]]=gt_utypes\n",
    "    print (\"gt_user\", len(gt_user)) #786\n",
    "\n",
    "\n",
    "    with open('data/graph_embd_yoga/data_des_@mention_em2.pickle', 'wb') as out_file:\n",
    "        pickle.dump([graph, id2name, name2id, type_start, type_end, user_start, user_end, des_start, des_end, \\\n",
    "                     annotated_des, annotated_user, twt2tokenized_text, weights_matrix, \\\n",
    "                     name2text, gt_user], out_file)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create_Graph() #create graph and validation set, glove weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of edges in graph from adjacency list\n",
    "f_in =open('data/graph_embd_yoga/yoga_graph_des_net_em2.adjlist', 'r')\n",
    "\n",
    "Lines = f_in.readlines()\n",
    " \n",
    "edges = 0\n",
    "nodes = 0\n",
    "for line in Lines:\n",
    "    clean_line = line.replace('\\n','')\n",
    "    clean_line = clean_line.replace('\\t','')\n",
    "    clean_line = clean_line.replace('\\r','')\n",
    "    edge_no_perline = len(clean_line.split(' ')) - 1 #edge number per line\n",
    "    edges = edges + edge_no_perline\n",
    "    nodes = nodes + 1\n",
    "    #print(count)\n",
    "print(\"total number of nodes : \", nodes) #33784\n",
    "print(\"total number of edges : \", edges) # 19927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
