{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "from numpy import array\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "torch.cuda.set_device(3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeInputData:\n",
    "\n",
    "    def __init__(self, batch_size=16):\n",
    "\n",
    "        with open('data/graph_embd_yoga/data_des_net.pickle', 'rb') as in_file:\n",
    "            [self.graph, self.id2name, self.name2id, self.type_start, self.type_end, \\\n",
    "             self.user_start, self.user_end, self.des_start, self.des_end, \\\n",
    "             self.annotated_des, self.annotated_user, \\\n",
    "             self.twt2tokenized_text, self.weights_matrix, self.name2text, self.gt_user]=pickle.load(in_file, encoding=\"bytes\")\n",
    "        \n",
    "        ## get the indecies of stances, issues, ....etc from data.pickle file also in graph.mapping file\n",
    "        self.all_types=set([i for i in range(self.type_start, self.type_end+1)])\n",
    "        #print(\"self.all_types\", self.all_types) #{0, 1, 2}\n",
    "        self.all_users=set([i for i in range(self.user_start, self.user_end+1)])\n",
    "        #print(\"self.all_users\", len(self.all_users)) #13301\n",
    "        self.all_des=set([i for i in range(self.des_start, self.des_end+1)])\n",
    "        #print(\"self.all_des\", len(self.all_des)) #13178\n",
    "        #self.all_loc=set([i for i in range(self.loc_start, self.loc_end+1)])\n",
    "        #print(\"self.all_loc\", len(self.all_loc)) #7302\n",
    "#         self.all_issue_bigrams=set([i for i in range(self.topic_bigram_start, self.topic_bigram_end+1)])\n",
    "#         #print(\"self.all_issue_bigrams\", len(self.all_issue_bigrams)) #{2000000, 2000001, 2000002,...}, len = 2039\n",
    "      \n",
    "#         self.all_indicator_labels=set([i for i in range(self.indicator_label_start, self.indicator_label_end+1)])\n",
    "#         #print('self.all_indicator_labels', self.all_indicator_labels) #{10}\n",
    "        \n",
    "        ###for positive example\n",
    "        self.user2type_adj_pos={}\n",
    "        self.user2des_adj_pos={}\n",
    "        self.user2loc_adj_pos={}\n",
    "        self.des2type_adj_pos={}\n",
    "        self.loc2type_adj_pos={}\n",
    "        self.user2mention_adj_pos={}\n",
    "        #self.issue2indicator_label_adj_pos={}\n",
    "        \n",
    "        ###for negative example\n",
    "        self.user2type_adj_neg={}\n",
    "        self.user2des_adj_neg={}\n",
    "        self.user2loc_adj_neg={}\n",
    "        self.des2type_adj_neg={}\n",
    "        self.loc2type_adj_neg={}\n",
    "        self.user2mention_adj_neg={}\n",
    "        #self.issue2indicator_label_adj_neg = {}\n",
    "\n",
    "\n",
    "        self.all_nodes_to_train=None\n",
    "\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    '''\n",
    "    Get nodes from graph\n",
    "    '''\n",
    "    def get_nodes(self, training_graph):\n",
    "\n",
    "        #print (\"training_graph\", len(training_graph)) #43301\n",
    "        '''\n",
    "        for each user we will create positive and negative examples\n",
    "        '''\n",
    "        for usr in self.all_users:\n",
    "            #print(usr) #100000..\n",
    "            adjacency_list=training_graph[usr]\n",
    "            #print(\"adjacency_list\", adjacency_list) #{1012088, 2002249}...\n",
    "            '''\n",
    "            pos/neg for user2type\n",
    "            '''\n",
    "            #print(adjacency_list & self.all_types, len(adjacency_list & self.all_types)) #set() len=0, {1} len = 1,... \n",
    "            if len(adjacency_list & self.all_types)>0:\n",
    "                self.user2type_adj_pos[usr]=adjacency_list & self.all_types\n",
    "                #print(\"pos\", self.user2type_adj_pos[usr]) #{1}\n",
    "                self.user2type_adj_neg[usr] = self.all_types - self.user2type_adj_pos[usr]\n",
    "                #print(\"neg\", self.user2type_adj_neg[usr]) #neg {0, 2}\n",
    "            '''\n",
    "            pos/neg for user2des\n",
    "            '''\n",
    "            ##each user connected to a description\n",
    "            #print(adjacency_list & self.all_des, len(adjacency_list & self.all_des)) #{1002822} len =1,\n",
    "            if len(adjacency_list & self.all_des)>0:\n",
    "                self.user2des_adj_pos[usr]=adjacency_list & self.all_des\n",
    "                #print('pos', self.user2des_adj_pos[usr])  #{1002822}\n",
    "                self.user2des_adj_neg[usr] = self.all_des - self.user2des_adj_pos[usr]\n",
    "                #print('neg', self.user2des_adj_neg[usr]) #rest of the funding entities are negative example for this particular ad\n",
    "                #sys.exit()\n",
    "\n",
    "            '''\n",
    "            pos/neg for user2mention\n",
    "            '''\n",
    "            if len(adjacency_list & self.all_users)>0:\n",
    "                self.user2mention_adj_pos[usr]=adjacency_list & self.all_users\n",
    "                #print('pos', self.user2mention_adj_pos[usr]) #{2000448, 2001843, 2000180, 2000725, 2001366, 2000991}\n",
    "                self.user2mention_adj_neg[usr] = self.all_users - self.user2mention_adj_pos[usr]\n",
    "                #print('neg', self.user2mention_adj_neg[usr]) #rest of issue bigrams are negative example\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        for each description we will create positive and negative examples\n",
    "        '''        \n",
    "        for des in self.all_des:\n",
    "            adjacency_list=training_graph[des]\n",
    "            '''\n",
    "            pos/neg for des2type\n",
    "            '''\n",
    "            if len(adjacency_list & self.all_types)>0:\n",
    "                self.des2type_adj_pos[des]=adjacency_list & self.all_types\n",
    "                self.des2type_adj_neg[des] = self.all_types - self.des2type_adj_pos[des]\n",
    "        \n",
    "         \n",
    "\n",
    "        self.all_nodes_to_train=[i for i in training_graph]\n",
    "\n",
    "        batches=[]\n",
    "        \n",
    "        discarded_nodes=0\n",
    "        shuffle(self.all_nodes_to_train)\n",
    "        print (\"all_nodes_to_train\", len(self.all_nodes_to_train)) #33784\n",
    "        j=0\n",
    "        while j<len(self.all_nodes_to_train):\n",
    "            batch_input = {'user2type': [], 'user2des': [],  'user2mention':[], \\\n",
    "                           'des2type': [] }\n",
    "            ##batch for positive example\n",
    "            batch_gold = {'user2type': [], 'user2des': [],  'user2mention':[],\\\n",
    "                           'des2type': [] }\n",
    "            ##batch for positive example\n",
    "            batch_neg = {'user2type': [], 'user2des': [], 'user2mention':[],\\\n",
    "                           'des2type': [] }\n",
    "\n",
    "            text_nodes=[]\n",
    "            \n",
    "            if j+self.batch_size<=len(self.all_nodes_to_train)-1:\n",
    "                nodes_to_train=self.all_nodes_to_train[j:j+self.batch_size]\n",
    "            else:\n",
    "                nodes_to_train = self.all_nodes_to_train[j:]\n",
    "\n",
    "            if len(nodes_to_train)==0:\n",
    "                continue\n",
    "                \n",
    "            for node in nodes_to_train:\n",
    "                '''\n",
    "                for each user to type/des/loc, we select positive and negative examples\n",
    "                '''\n",
    "                if node in self.all_users: \n",
    "                    text_nodes.append(node)\n",
    "                    \n",
    "                    '''\n",
    "                    user2type\n",
    "                    '''\n",
    "                    if node in self.user2type_adj_pos:\n",
    "                        #print (\"node\", node) #106497\n",
    "                        #print(self.user2type_adj_pos[node], len(self.user2type_adj_pos[node])) #{0}, len=1\n",
    "                        #print('prev b ', batch_input['ad2stance'])\n",
    "                        batch_input['user2type']+=[node for i in range(0,len(self.user2type_adj_pos[node]))]\n",
    "                        #print('later b ', batch_input['user2type']) #[106497]\n",
    "                        #print('prev p', batch_gold['ad2stance'])\n",
    "                        batch_gold['user2type']+=list(self.user2type_adj_pos[node])\n",
    "                        #print('later p', batch_gold['ad2stance']) #[0], multi-label [0, 0, 3]\n",
    "                        #print('prev n', batch_neg['ad2stance'])\n",
    "                        #print('random ', random.sample(self.ad2stance_adj_neg[node], 2))\n",
    "                        #print('len pos', len(self.user2type_adj_pos[node])) #1\n",
    "                        ##randomly select two negative example for each positive example\n",
    "                        batch_neg['user2type']+=[random.sample(self.user2type_adj_neg[node], 2) for i in range(0, len(self.user2type_adj_pos[node]))]\n",
    "                        #print('later n', batch_neg['user2type']) #[[4, 1]], multilabel [[4, 1], [4, 2], [2, 1]]\n",
    "                    \n",
    "                    '''\n",
    "                    user2des\n",
    "                    '''\n",
    "                    if node in self.user2des_adj_pos:\n",
    "                        #print ('ad2fe', node)\n",
    "                        batch_input['user2des']+=[node for i in range(0,len(self.user2des_adj_pos[node]))]\n",
    "                        batch_gold['user2des']+=list(self.user2des_adj_pos[node])\n",
    "                        batch_neg['user2des']+=[random.sample(self.user2des_adj_neg[node], 5) for i in range(0, len(self.user2des_adj_pos[node]))]\n",
    "\n",
    "\n",
    "                    '''\n",
    "                    user2mention\n",
    "                    '''\n",
    "                    if node in self.user2mention_adj_pos:\n",
    "                        \n",
    "                        batch_input['user2mention'] += [node for i in range(0, len(self.user2mention_adj_pos[node]))]\n",
    "                        batch_gold['user2mention'] += list(self.user2mention_adj_pos[node])\n",
    "                        batch_neg['user2mention'] += [random.sample(self.user2mention_adj_neg[node], 5) for i in range(0, len(self.user2mention_adj_pos[node]))]\n",
    "\n",
    "                '''\n",
    "                for each description to type, we select positive and negative examples\n",
    "                '''\n",
    "                if node in self.all_des:\n",
    "                    if node in self.des2type_adj_pos:\n",
    "                        #print ('fe2st', node)\n",
    "                        batch_input['des2type']+=[node for i in range(0,len(self.des2type_adj_pos[node]))]\n",
    "                        batch_gold['des2type']+=list(self.des2type_adj_pos[node])\n",
    "                        batch_neg['des2type']+=[random.sample(self.des2type_adj_neg[node], 2) for i in range(0, len(self.des2type_adj_pos[node]))]\n",
    "\n",
    "                        \n",
    "            #print(\"j, self.batch_size\", j, self.batch_size) #100\n",
    "            for k in batch_input:\n",
    "                #print(\"k\", k) #ad2stance, ad2funding_entity\n",
    "                batch_input[k] = array(batch_input[k])\n",
    "                #print(\"batch_input[k]\", batch_input[k]) #[104586 135113 119709 127739 130641 113722 122613 123000 123000 123074 100555 100555 100165 102666 135542 127619 118428]\n",
    "            for k in batch_gold:\n",
    "                batch_gold[k] = array([batch_gold[k]]).transpose()\n",
    "                #print('batch_gold[k]', batch_gold[k])\n",
    "            for k in batch_neg:\n",
    "                batch_neg[k] = array(batch_neg[k])\n",
    "                #print('batch_neg[k]', batch_neg[k])\n",
    "\n",
    "            j=j+self.batch_size\n",
    "            batches.append([batch_input, batch_gold, batch_neg, text_nodes])\n",
    "            #print(\"batches\", batches)\n",
    "            \n",
    "        \n",
    "        self.batches=batches\n",
    "        \n",
    "        print (\"Discarded: %d nodes.\"%(discarded_nodes))\n",
    "    \n",
    "    def save_embeddings(self, model):\n",
    "\n",
    "        #output_dir='../../scratch/fb_ads_data/output/embeddings/'\n",
    "        \n",
    "        #f=open(output_dir+\"stance.embeddings\",\"w\")\n",
    "        f=open(\"data/graph_embd_yoga/output/des_net/utype.embeddings\",\"w\")\n",
    "        utype_embd=model.utype_embeddings.weight.cpu().data.numpy()\n",
    "        utype_embd=utype_embd.tolist()\n",
    "        for i in range(0,len(utype_embd)):\n",
    "            id=i+self.type_start\n",
    "            name=self.id2name[id]\n",
    "            f.write(str(name))\n",
    "            embd=utype_embd[i]\n",
    "            for j in range(len(embd)):\n",
    "                if j==0:\n",
    "                    f.write(\"\\t\"+str(embd[j]))\n",
    "                else:\n",
    "                    f.write(\" \"+str(embd[j]))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        f = open(\"data/graph_embd_yoga/output/des_net/user.embeddings\", \"w\")\n",
    "        users = [k for k in range(self.user_start, self.user_end + 1)]\n",
    "        for usr in users:\n",
    "            embd = model.user_embeddings([usr])\n",
    "            embd = embd[0]\n",
    "            embd = embd.cpu().data.numpy()\n",
    "            embd = embd.tolist()\n",
    "            name = self.id2name[usr]\n",
    "            f.write(str(name))\n",
    "            for j in range(len(embd)):\n",
    "                if j == 0:\n",
    "                    f.write(\"\\t\" + str(embd[j]))\n",
    "                else:\n",
    "                    f.write(\" \" + str(embd[j]))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "        f = open(\"data/graph_embd_yoga/output/des_net/description.embeddings\", \"w\")          \n",
    "        des_embd = model.des_embeddings.weight.cpu().data.numpy()\n",
    "        des_embd = des_embd.tolist()\n",
    "        for i in range(0, len(des_embd)):\n",
    "            id = i + self.des_start\n",
    "            name = self.id2name[id]\n",
    "            f.write(str(name))\n",
    "            embd = des_embd[i]\n",
    "            for j in range(len(embd)):\n",
    "                if j == 0:\n",
    "                    f.write(\"\\t\" + str(embd[j]))\n",
    "                else:\n",
    "                    f.write(\" \" + str(embd[j]))\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f = open(\"data/graph_embd_yoga/output/des_net/network.embeddings\", \"w\")          \n",
    "        net_embd = model.net_embeddings.weight.cpu().data.numpy()\n",
    "        net_embd = net_embd.tolist()\n",
    "        for i in range(0, len(net_embd)):\n",
    "            id = i + self.user_start\n",
    "            name = self.id2name[id]\n",
    "            f.write(str(name))\n",
    "            embd = net_embd[i]\n",
    "            for j in range(len(embd)):\n",
    "                if j == 0:\n",
    "                    f.write(\"\\t\" + str(embd[j]))\n",
    "                else:\n",
    "                    f.write(\" \" + str(embd[j]))\n",
    "            f.write(\"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Bi-LSTM \n",
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, non_trainable=True, cuda_available=True):\n",
    "        super(BLSTM, self).__init__()\n",
    "\n",
    "        with open('data/graph_embd_yoga/data_des_net.pickle', 'rb') as in_file:\n",
    "            [self.graph, self.id2name, self.name2id, self.type_start, self.type_end, \\\n",
    "             self.user_start, self.user_end, self.des_start, self.des_end, \\\n",
    "             self.annotated_des,  self.annotated_user, \\\n",
    "             self.twt2tokenized_text, self.weights_matrix, self.name2text, self.gt_user]=pickle.load(in_file, encoding=\"bytes\")\n",
    "        \n",
    "\n",
    "        self.hidden_dim = 150\n",
    "        num_embeddings = len(self.weights_matrix)\n",
    "        embedding_dim = len(self.weights_matrix[0])\n",
    "        self.cuda_available=cuda_available\n",
    "        #if torch.cuda.is_available():\n",
    "        if self.cuda_available:\n",
    "            self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0).cuda()\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.word_embeddings.weight.data.copy_(torch.from_numpy(self.weights_matrix))\n",
    "\n",
    "        if non_trainable:\n",
    "            self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        #if torch.cuda.is_available():\n",
    "        if self.cuda_available:\n",
    "            #self.lstm = nn.LSTM(300, self.hidden_dim, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "            self.lstm = nn.LSTM(300, self.hidden_dim, num_layers = 1, bidirectional=True, batch_first=True, dropout = 0).cuda()\n",
    "            \n",
    "        else:\n",
    "            #self.lstm = nn.LSTM(300, self.hidden_dim, bidirectional=True, batch_first=True, dropout=0.5)\n",
    "            self.lstm = nn.LSTM(300, self.hidden_dim, num_layers = 1, bidirectional=True, batch_first=True, dropout = 0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        #if torch.cuda.is_available():\n",
    "        if self.cuda_available:\n",
    "            return (torch.zeros(2, batch_size, self.hidden_dim).cuda(),\n",
    "                    torch.zeros(2, batch_size, self.hidden_dim).cuda())\n",
    "        return (torch.zeros(2, batch_size, self.hidden_dim),\n",
    "                torch.zeros(2, batch_size, self.hidden_dim))\n",
    "    \n",
    "\n",
    "    ###to get rid of empty X\n",
    "    def get_padded(self, X):\n",
    "        #print (\"X\", X)\n",
    "        X_lengths = []\n",
    "        new_X = []\n",
    "        for sentence in X:\n",
    "            #print('blstm', sentence)\n",
    "            if len(sentence) > 0: \n",
    "                X_lengths.append(len(sentence)) \n",
    "                new_X.append(sentence)\n",
    "        \n",
    "        pad_token = 0\n",
    "        longest_sent = max(X_lengths)\n",
    "        batch_size = len(new_X)\n",
    "        padded_X = np.ones((batch_size, longest_sent)) * pad_token\n",
    "\n",
    "        for i, x_len in enumerate(X_lengths):\n",
    "            sequence = new_X[i]\n",
    "            padded_X[i, 0:x_len] = sequence[:x_len]\n",
    "        #print (\"padded_X\", padded_X)\n",
    "        return padded_X, X_lengths\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        #print (\"inside forward Bi-LSTM \", X, len(X)) #[122211, 105694, 128324, 123265, 123692, 110528, 104453..\n",
    "        # ---------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len, embedding_dim)\n",
    "        X = [self.twt2tokenized_text[seg_id] for seg_id in X]\n",
    "        #print (X, len(X)) \n",
    "        X, X_lengths=self.get_padded(X)\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden(len(X))\n",
    "        # batch_size, seq_len = len(X), len(X[0])\n",
    "        #print (\"X, X_lengths\", X, X_lengths)\n",
    "\n",
    "        #if torch.cuda.is_available():\n",
    "        if self.cuda_available:\n",
    "            X = self.word_embeddings(Variable(torch.cuda.LongTensor(np.array(X))))\n",
    "        else:\n",
    "            X = self.word_embeddings(Variable(torch.LongTensor(np.array(X))))\n",
    "\n",
    "        seg_sort_index = sorted(range(len(X_lengths)), key=lambda k: X_lengths[k], reverse=True)\n",
    "        seg_sort_index_map = {old: new for new, old in enumerate(seg_sort_index)}\n",
    "        reverse_seg_index = [seg_sort_index_map[i] for i in range(len(seg_sort_index))]\n",
    "        reverse_seg_index_var = torch.LongTensor(reverse_seg_index)\n",
    "\n",
    "        #if torch.cuda.is_available():\n",
    "        if self.cuda_available:\n",
    "            #X_lengths = X_lengths.cuda()\n",
    "            reverse_seg_index_var = reverse_seg_index_var.cuda()\n",
    "\n",
    "        seg_lengths_sort = sorted(X_lengths, reverse=True)\n",
    "        # de-concat the document sentences in the whole batch\n",
    "        #print X\n",
    "        X_sort = torch.cat([X[i].unsqueeze(0) for i in seg_sort_index], 0)\n",
    "        #print (X_sort, seg_lengths_sort)\n",
    "        #print (\"X_sort, seg_lengths_sort\", len(X_sort), len(seg_lengths_sort))\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(X_sort, seg_lengths_sort, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        \n",
    "        #print list(X.size()), reverse_seg_index_var\n",
    "\n",
    "        #if torch.cuda.is_available():\n",
    "        if self.cuda_available:\n",
    "            seg_embeds = torch.index_select(X, 0, reverse_seg_index_var).sum(1) / torch.cuda.FloatTensor(array(X_lengths)).view((-1, 1))\n",
    "        else:\n",
    "            seg_embeds = torch.index_select(X, 0, reverse_seg_index_var).sum(1) / torch.FloatTensor(array(X_lengths)).view((-1, 1))\n",
    "\n",
    "        return seg_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######This is our model for embedding\n",
    "\n",
    "class Embedder(torch.nn.Module):\n",
    "    def __init__(self, cuda_available=True):\n",
    "        super(Embedder, self).__init__()\n",
    "\n",
    "        with open('data/graph_embd_yoga/data_des_net.pickle', 'rb') as in_file:\n",
    "            [self.graph, self.id2name, self.name2id, self.type_start, self.type_end, \\\n",
    "             self.user_start, self.user_end, self.des_start, self.des_end, \\\n",
    "             self.annotated_des,  self.annotated_user, \\\n",
    "             self.twt2tokenized_text, self.weights_matrix, self.name2text, self.gt_user]=pickle.load(in_file, encoding=\"bytes\")\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "        self.type_count=self.type_end-self.type_start+1\n",
    "        #print('self.type_count', self.type_count) #5\n",
    "        self.user_count=self.user_end-self.user_start+1\n",
    "        #print('self.user_count', self.user_count) #35855\n",
    "        self.des_count=self.des_end-self.des_start+1\n",
    "        #print('self.des_count', self.des_count) #5387\n",
    "        #self.loc_count=self.loc_end-self.loc_start+1\n",
    "        #print('self.loc_count', self.loc_count) #2039\n",
    "        #self.indicator_label_count=self.indicator_label_end-self.indicator_label_start+1\n",
    "        #print('self.indicator_label_count', self.indicator_label_count) #1\n",
    "\n",
    "        self.cuda_available=cuda_available\n",
    "        self.embedding_size = 300\n",
    "\n",
    "\n",
    "        self.scale = 1.0 / math.sqrt(self.embedding_size)\n",
    "        \n",
    "        self.embed_rng = np.random.RandomState(1)\n",
    "        \n",
    "        if self.cuda_available:\n",
    "            self.utype_embeddings = nn.Embedding(self.type_count, self.embedding_size).cuda()\n",
    "            self.des_embeddings = nn.Embedding(self.des_count, self.embedding_size).cuda()\n",
    "            #self.loc_embeddings = nn.Embedding(self.loc_count, self.embedding_size).cuda()\n",
    "            self.net_embeddings = nn.Embedding(self.user_count, self.embedding_size).cuda()\n",
    "            \n",
    "\n",
    "        else:\n",
    "            self.utype_embeddings = nn.Embedding(self.type_count, self.embedding_size)\n",
    "            self.des_embeddings = nn.Embedding(self.des_count, self.embedding_size)\n",
    "            #self.loc_embeddings = nn.Embedding(self.loc_count, self.embedding_size)\n",
    "            self.net_embeddings = nn.Embedding(self.user_count, self.embedding_size)\n",
    "        \n",
    "        \n",
    "        type_embed_values = self.embed_rng.normal(scale=self.scale, size=(self.type_count, self.embedding_size))\n",
    "        des_embed_values = self.embed_rng.normal(scale=self.scale, size=(self.des_count, self.embedding_size))\n",
    "        #loc_embed_values = self.embed_rng.normal(scale=self.scale, size=(self.loc_count, self.embedding_size))\n",
    "        net_embed_values = self.embed_rng.normal(scale=self.scale, size=(self.user_count, self.embedding_size))\n",
    "        \n",
    "        self.utype_embeddings.weight.data.copy_(torch.from_numpy(type_embed_values))\n",
    "        self.des_embeddings.weight.data.copy_(torch.from_numpy(des_embed_values))\n",
    "        #self.loc_embeddings.weight.data.copy_(torch.from_numpy(loc_embed_values))\n",
    "        self.net_embeddings.weight.data.copy_(torch.from_numpy(net_embed_values))\n",
    "        \n",
    "    \n",
    "    \n",
    "        if self.cuda_available:\n",
    "            self.user_embeddings = BLSTM(self.cuda_available).cuda()\n",
    "        else:\n",
    "            self.user_embeddings = BLSTM(self.cuda_available)\n",
    "\n",
    "\n",
    "        self.CrossEntropyLoss = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        #self.aspects = ['ad2stance', 'ad2funding_entity', 'ad2issue_bigram',\\\n",
    "                        #'funding_entity2stance', 'issue_bigram2issue', 'issue2indicator_label']\n",
    "        \n",
    "        self.aspects = ['user2type', 'user2des', 'user2mention',\\\n",
    "                        'des2type']\n",
    "\n",
    "                        \n",
    "        #print('self.aspects ', self.aspects )                \n",
    "    def decision(self, probability):\n",
    "        return random.random() < probability\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #print(\"batch\", batch)\n",
    "        batch_input_index = batch[0]\n",
    "        #print(\"batch_input_index\", batch_input_index) #len = 6, dictionary\n",
    "        batch_gold_index = batch[1]\n",
    "        #print(\"batch_gold_index\", batch_gold_index)\n",
    "        batch_negs_index = batch[2]\n",
    "        #print(\"batch_negs_index\", batch_negs_index)\n",
    "        text_nodes = batch[3]\n",
    "        #print('text_nodes', text_nodes, type(text_nodes), len(text_nodes)) #list #30\n",
    "#         text_ele = [self.twt2tokenized_text[seg_id] for seg_id in text_nodes_orig]\n",
    "#         #print('text_ele', text_ele, len(text_ele))\n",
    "# #         for sentence in text_ele:\n",
    "# #             print('embed sent', sentence)\n",
    "#         text_nodes = []\n",
    "#         for i in range (0, len(text_nodes_orig)):\n",
    "#             for sentence in text_ele:\n",
    "#                 if len(sentence) > 0: \n",
    "#                     text_nodes.append(text_nodes_orig[i])\n",
    "#         print('text_nodes', text_nodes, type(text_nodes), len(text_nodes))\n",
    "        \n",
    "        \n",
    "       \n",
    "        if len(text_nodes) > 0:\n",
    "            textid2index = {}\n",
    "            \n",
    "#             for i in range(0, len(text_nodes)):\n",
    "#                 textid2index[text_nodes[i]] = i\n",
    "            if self.cuda_available:\n",
    "                '''\n",
    "                hlstm only\n",
    "                '''\n",
    "                output_doc_embeddings = self.user_embeddings(text_nodes).cuda()\n",
    "                #print(\"output_doc_embeddings\", output_doc_embeddings)\n",
    "                #print(\"size output_doc_embeddings\", output_doc_embeddings.size())\n",
    "\n",
    "            else:\n",
    "                '''\n",
    "                hlstm only\n",
    "                '''\n",
    "                output_doc_embeddings = self.user_embeddings(text_nodes)\n",
    "                #print(\"size output_doc_embeddings\", output_doc_embeddings.size())\n",
    "            \n",
    "            for i in range(0, output_doc_embeddings.size(0)):\n",
    "                textid2index[text_nodes[i]] = i\n",
    "                \n",
    "\n",
    "\n",
    "        embedding_size = self.embedding_size\n",
    "        batch_target_index = {}\n",
    "        batch_input = {}\n",
    "        batch_target = {}\n",
    "        input_embed = {}\n",
    "        target_embed = {}\n",
    "        sim_score = {}\n",
    "        loss_all = {}\n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        \n",
    "        for aspect in self.aspects:\n",
    "            #print('loop aspect', aspect)\n",
    "            #print(\"len issue2indicator_label : \", len(batch_gold_index['issue2indicator_label'])) # 0 why??\n",
    "            #print(\"len funding_entity2stance : \", len(batch_gold_index['funding_entity2stance'])) #0\n",
    "            if (len(batch_gold_index[aspect]) == 0):\n",
    "                continue\n",
    "\n",
    "\n",
    "            if (aspect == 'user2type'):\n",
    "                target_embeddings = self.utype_embeddings\n",
    "            elif (aspect == 'user2des'):\n",
    "                target_embeddings = self.des_embeddings\n",
    "#             elif (aspect == 'user2loc'):\n",
    "#                 target_embeddings = self.loc_embeddings\n",
    "            elif (aspect == 'user2mention'):\n",
    "                target_embeddings = self.net_embeddings\n",
    "            elif (aspect == 'des2type'):\n",
    "                target_embeddings = self.utype_embeddings\n",
    "                #print(\"target_embeddings funding_entity2stance\", target_embeddings) #(5, 300)\n",
    "#             elif (aspect == 'loc2type'):\n",
    "#                 target_embeddings = self.utype_embeddings \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "            if (aspect == 'user2type'):\n",
    "                if self.cuda_available:\n",
    "                    #index = [textid2index[idx] for idx in batch_input_index[aspect]]\n",
    "                    index = []\n",
    "                    for idx in batch_input_index[aspect]:\n",
    "                        if idx in textid2index:\n",
    "                            index.append(textid2index[idx])\n",
    "                    batch_input[aspect] = output_doc_embeddings[index]\n",
    "                    #print(\"ad2stance\", batch_gold_index[aspect])\n",
    "                    #print(\"self.stance_start\", self.stance_start)\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.cuda.LongTensor(batch_gold_index[aspect] - self.type_start)), Variable(\n",
    "                        torch.cuda.LongTensor(batch_negs_index[aspect] - self.type_start))), 1)\n",
    "                else:\n",
    "                    #index = [textid2index[idx] for idx in batch_input_index[aspect]]\n",
    "                    index = []\n",
    "                    for idx in batch_input_index[aspect]:\n",
    "                        if idx in textid2index:\n",
    "                            index.append(textid2index[idx])\n",
    "                    batch_input[aspect] = output_doc_embeddings[index]\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.LongTensor(batch_gold_index[aspect] - self.type_start)), Variable(\n",
    "                        torch.LongTensor(batch_negs_index[aspect] - self.type_start))), 1)\n",
    "\n",
    "            elif (aspect == 'user2des'):\n",
    "                if self.cuda_available:\n",
    "                    #print('batch_input_index[aspect]', batch_input_index[aspect], len(batch_input_index[aspect]))\n",
    "                    #print('textid2index', textid2index, len(textid2index))\n",
    "                    #index = [textid2index[idx] for idx in batch_input_index[aspect]]\n",
    "                    index = []\n",
    "                    for idx in batch_input_index[aspect]:\n",
    "                        if idx in textid2index:\n",
    "                            index.append(textid2index[idx])\n",
    "                    #print('index', index, len(index)) ## 0...25\n",
    "                    #print('output_doc_embeddings', output_doc_embeddings.size()) # 25*300\n",
    "                    batch_input[aspect] = output_doc_embeddings[index]\n",
    "                    #print('batch_input ', batch_input[aspect])\n",
    "                    #print(\"ad2funding_entity\", batch_gold_index[aspect])\n",
    "                    #print(\"self.funding_entity_start\", self.funding_entity_start)\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.cuda.LongTensor(batch_gold_index[aspect] - self.des_start)), Variable(\n",
    "                        torch.cuda.LongTensor(batch_negs_index[aspect] - self.des_start))), 1)\n",
    "                else:\n",
    "                    #print('batch_input_index ', batch_input_index[aspect], len(batch_input_index[aspect]))\n",
    "                    #print('textid2index', textid2index, len(textid2index))\n",
    "                    #index = [textid2index[idx] for idx in batch_input_index[aspect]]\n",
    "                    index = []\n",
    "                    for idx in batch_input_index[aspect]:\n",
    "                        if idx in textid2index:\n",
    "                            index.append(textid2index[idx])\n",
    "                    #print('index', index, len(index)) ## 0...25\n",
    "                    #print('output_doc_embeddings', output_doc_embeddings.size()) # 25*300\n",
    "                    batch_input[aspect] = output_doc_embeddings[index]\n",
    "                    #print('batch_input ', batch_input[aspect])\n",
    "#                     print(\"ad2funding_entity\", batch_gold_index[aspect])\n",
    "#                     print(\"self.funding_entity_start\", self.funding_entity_start)\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.LongTensor(batch_gold_index[aspect] - self.des_start)), Variable(\n",
    "                        torch.LongTensor(batch_negs_index[aspect] - self.des_start))), 1)\n",
    "\n",
    "\n",
    "\n",
    "            elif (aspect == 'user2mention'):\n",
    "                if self.cuda_available:\n",
    "                    #index = [textid2index[idx] for idx in batch_input_index[aspect]]\n",
    "                    index = []\n",
    "                    for idx in batch_input_index[aspect]:\n",
    "                        if idx in textid2index:\n",
    "                            index.append(textid2index[idx])\n",
    "                    batch_input[aspect] = output_doc_embeddings[index]\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.cuda.LongTensor(batch_gold_index[aspect] - self.user_start)), Variable(\n",
    "                        torch.cuda.LongTensor(batch_negs_index[aspect] - self.user_start))), 1)\n",
    "                else:\n",
    "                    #index = [textid2index[idx] for idx in batch_input_index[aspect]]\n",
    "                    index = []\n",
    "                    for idx in batch_input_index[aspect]:\n",
    "                        if idx in textid2index:\n",
    "                            index.append(textid2index[idx])\n",
    "                    batch_input[aspect] = output_doc_embeddings[index]\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.LongTensor(batch_gold_index[aspect] - self.user_start)), Variable(\n",
    "                        torch.LongTensor(batch_negs_index[aspect] - self.user_start))), 1)\n",
    "                    \n",
    "            elif (aspect == 'des2type'):\n",
    "                if self.cuda_available:\n",
    "                    batch_input[aspect] = self.des_embeddings(\n",
    "                        Variable(torch.cuda.LongTensor(batch_input_index[aspect] - self.des_start))).view(\n",
    "                        (-1, self.embedding_size))\n",
    "                    #print(\"funding_entity2stance\", batch_gold_index[aspect])\n",
    "                    #print(\"self.stance_start\", self.stance_start)\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.cuda.LongTensor(batch_gold_index[aspect] - self.type_start)), Variable(\n",
    "                        torch.cuda.LongTensor(batch_negs_index[aspect] - self.type_start))), 1)\n",
    "                else:\n",
    "                    batch_input[aspect] = self.des_embeddings(\n",
    "                        Variable(torch.LongTensor(batch_input_index[aspect] - self.des_start))).view(\n",
    "                        (-1, self.embedding_size))\n",
    "                    batch_target_index[aspect] = torch.cat((Variable(\n",
    "                        torch.LongTensor(batch_gold_index[aspect] - self.type_start)), Variable(\n",
    "                        torch.LongTensor(batch_negs_index[aspect] - self.type_start))), 1)\n",
    "\n",
    "\n",
    "            if aspect in ['user2des', 'user2mention']:\n",
    "                #print(\"aspect\", aspect)\n",
    "                example_size = 5 + 1\n",
    "                #print(\"batch_target_index funding_entity2stance\", len(batch_target_index) )\n",
    "                #print(\"batch_target_index[funding_entity2stance]\", batch_target_index[aspect])\n",
    "\n",
    "            else:\n",
    "                #print(\"else \", aspect)\n",
    "                example_size = 2 + 1\n",
    "\n",
    "            #if aspect in batch_target_index :\n",
    "            batch_target[aspect] = target_embeddings(batch_target_index[aspect]).view((-1, example_size, self.embedding_size))\n",
    "            #print(\"batch_target \", batch_target[aspect], batch_target[aspect].size())\n",
    "            dropout = nn.Dropout(p=0.7)\n",
    "            input_layers = [dropout(batch_input[aspect])]\n",
    "            input_embed[aspect] = input_layers[-1]\n",
    "\n",
    "            target_layers = [dropout(batch_target[aspect])]\n",
    "            target_embed[aspect] = target_layers[-1]\n",
    "            \n",
    "#             print(\"\\n * target_embed[aspect] \\n \")\n",
    "            #print('target_embed', target_embed[aspect].size()) #torch.Size([13, 6, 300]), torch.Size([3, 3, 300]), torch.Size([15, 6, 300])\n",
    "\n",
    "#             print(target_embed[aspect].device)\n",
    "\n",
    "#             print(\"\\n * input_embed[aspect] \\n \")\n",
    "#             print('input_embed', input_embed[aspect].size()) #torch.Size([13, 300]), torch.Size([3, 300]), , torch.Size([15, 300])\n",
    "#             print(input_embed[aspect].device)\n",
    "            #print(\"input_embed[aspect].view \", input_embed[aspect].view(-1, embedding_size, 1).size()) #torch.Size([1, 300, 1]), torch.Size([12, 300, 1]), torch.Size([14, 300, 1]) ??\n",
    "\n",
    "            #print('bmm', torch.bmm(target_embed[aspect],input_embed[aspect].view(-1, embedding_size, 1)).size())\n",
    "            #print('example_size', example_size)\n",
    "            #print('bmm view', torch.bmm(target_embed[aspect],input_embed[aspect].view(-1, embedding_size, 1)).view(-1, example_size).size())\n",
    "            sim_score[aspect] = torch.bmm(\n",
    "                target_embed[aspect],\n",
    "                input_embed[aspect].view(-1, embedding_size, 1)).view(-1, example_size)\n",
    "\n",
    "            if self.cuda_available:\n",
    "                target = Variable(torch.cuda.LongTensor(batch_input[aspect].size(0)).zero_())\n",
    "            else:\n",
    "                target = Variable(torch.LongTensor(batch_input[aspect].size(0)).zero_())\n",
    "\n",
    "            self.CrossEntropyLoss = nn.CrossEntropyLoss(reduction='mean')\n",
    "            \n",
    "            #print('sim_score ', sim_score[aspect], sim_score[aspect].size())\n",
    "            #print('target ', target, target.size())\n",
    "            loss_all[aspect] = self.CrossEntropyLoss(sim_score[aspect], target)\n",
    "\n",
    "            loss_all[aspect] = torch.sum(loss_all[aspect])\n",
    "\n",
    "            rate = 1.0\n",
    "\n",
    "            loss += loss_all[aspect] * rate\n",
    "            #print(\"loop loss\", loss)\n",
    "        \n",
    "        return loss, loss_all\n",
    "    \n",
    "    def chunks(self, l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i + n] for i in range(0, len(l), n))\n",
    "\n",
    "    def dot_product(self, x, y):\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        return np.dot(x, y)\n",
    "\n",
    "    def predict_utype(self):\n",
    "        batch_input_index = [id for id in self.gt_user]\n",
    "        batch_gold=[self.gt_user[id] for id in self.gt_user]\n",
    "\n",
    "        utype_embd = self.utype_embeddings.weight.cpu().data.numpy()\n",
    "        utype_embd = utype_embd.tolist()\n",
    "        batch_input_index = self.chunks(batch_input_index, 30)\n",
    "        doc_embds = []\n",
    "        for chunk in batch_input_index:\n",
    "            embd = self.user_embeddings(chunk)\n",
    "            embd = (embd.cpu().data.numpy()).tolist()\n",
    "            doc_embds += embd\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(0, len(doc_embds)):\n",
    "            d_embd = doc_embds[i]\n",
    "            scores = []\n",
    "            for l in range(0, len(utype_embd)):\n",
    "                l_embd = utype_embd[l]\n",
    "                scores.append(self.dot_product(d_embd, l_embd))\n",
    "\n",
    "            predictions.append([scores.index(max(scores))])\n",
    "\n",
    "\n",
    "        correct=0\n",
    "        #preds = []\n",
    "#         print('len(predictions)', len(predictions), type(predictions))\n",
    "#         print('predictions[i]', predictions[0], predictions[1], type(predictions[0]), type (predictions[1]),  predictions[0][0], type(predictions[0][0]))\n",
    "#         print('len(batch_gold)', len(batch_gold))\n",
    "#         print('batch_gold[i]', batch_gold[0], batch_gold[1], type(batch_gold[0]), type(batch_gold[1]))\n",
    "        for i in range(len(predictions)):\n",
    "            #if len(set(predictions[i]) & set(batch_gold[i]))>0:\n",
    "            if predictions[i][0] == batch_gold[i] :\n",
    "                correct+=1\n",
    "            #preds.append(predictions[i][0])\n",
    "\n",
    "\n",
    "        print (\"accuracy:\", correct/float(len(predictions)))\n",
    "        from sklearn.metrics import f1_score\n",
    "        macro_f1 = f1_score(batch_gold, predictions, average='macro')\n",
    "        print(\"macro_f1\", macro_f1)\n",
    "        \n",
    "    def predict_utype_all_des(self):\n",
    "        des_embds=self.des_embeddings.weight.cpu().data.numpy()\n",
    "        des_embds=des_embds.tolist()\n",
    "            \n",
    "        utype_embd = self.utype_embeddings.weight.cpu().data.numpy()\n",
    "        utype_embd = utype_embd.tolist()\n",
    "\n",
    "\n",
    "        des2utype={}\n",
    "        for i in range(0, len(des_embds)):\n",
    "            des_embd = des_embds[i]\n",
    "            scores = []\n",
    "            for l in range(0, len(utype_embd)):\n",
    "                l_embd = utype_embd[l]\n",
    "                scores.append(self.dot_product(des_embd, l_embd))\n",
    "\n",
    "            utype2score={}\n",
    "            for j in range(len(scores)):\n",
    "                utype2score[self.id2name[j+self.type_start]]=scores[j]\n",
    "\n",
    "            sorted_utype={k: v for k, v in sorted(utype2score.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "            des2utype[self.id2name[i+self.des_start]]=sorted_utype\n",
    "\n",
    "        return des2utype\n",
    "\n",
    "    def predict_utype_all_users(self):\n",
    "        _batch_input_index = [id for id in range(self.user_start, self.user_end + 1)]\n",
    "        # batch_gold = [self.ad2annotated_issue[id] for id in self.ad2annotated_issue]\n",
    "\n",
    "        utype_embd = self.utype_embeddings.weight.cpu().data.numpy()\n",
    "        utype_embd = utype_embd.tolist()\n",
    "        batch_input_index = self.chunks(_batch_input_index, 30)\n",
    "        user_embds = []\n",
    "        for chunk in batch_input_index:\n",
    "            embd = self.user_embeddings(chunk)\n",
    "            embd = (embd.cpu().data.numpy()).tolist()\n",
    "            user_embds += embd\n",
    "\n",
    "        user2utype = {}\n",
    "        for i in range(0, len(user_embds)):\n",
    "            user_embd = user_embds[i]\n",
    "            scores = []\n",
    "            for l in range(0, len(utype_embd)):\n",
    "                l_embd = utype_embd[l]\n",
    "                scores.append(self.dot_product(user_embd, l_embd))\n",
    "\n",
    "            utype2score = {}\n",
    "            for j in range(len(scores)):\n",
    "                utype2score[self.id2name[j + self.type_start]] = scores[j]\n",
    "\n",
    "            sorted_utype = {k: v for k, v in sorted(utype2score.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "            user2utype[self.id2name[_batch_input_index[i]]] = sorted_utype\n",
    "\n",
    "        return user2utype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NID=NodeInputData(batch_size=16)\n",
    "print (\"Creating batches...\")\n",
    "NID.get_nodes(NID.graph)\n",
    "print (\"Batches Created!\")\n",
    "# global cuda_available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "# #print(cuda_available)\n",
    "\n",
    "print (\"Initializing the model...\")\n",
    "\n",
    "model = Embedder(cuda_available=True)\n",
    "#model = Embedder(cuda_available=torch.cuda.is_available())\n",
    "best_model = Embedder(cuda_available=True)\n",
    "model2 = Embedder(cuda_available=True)\n",
    "\n",
    "print (\"Model Initiated!\")\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "print (\"Total Batches: %d\"%len(NID.batches)) #Total Batches: 434 if 100, Total Batches: 1354 if 32\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name)\n",
    "        \n",
    "# for name, param in model1.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name)\n",
    "\n",
    "min_loss=10000000\n",
    "no_up=0\n",
    "model.train()\n",
    "for epoch in range(0, 100): #initiate number of epoches\n",
    "    start=time.time()\n",
    "    average_loss=0\n",
    "    count=0\n",
    "    for batch in NID.batches:\n",
    "        #print(\"i\", i)\n",
    "        #print (\"batch\" , batch, type(batch), len(batch)) # list, 4\n",
    "        #print (\"batch len\" , len(batch[0]), len(batch[1]), len(batch[2]), len(batch[3])) # 5 5 5 9, 5 5 5 16\n",
    "        #print (\"gold batch\" , batch[1], type(batch[1]), len(batch[1])) #<class 'dict'>, 6\n",
    "        optimizer.zero_grad()\n",
    "        l,loss=model(batch)\n",
    "        #print(\"l for loss on epoch : \", l, epoch)\n",
    "        if l==0:\n",
    "            continue\n",
    "            \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        #average_loss+=l.data[0]\n",
    "        average_loss+=l.data\n",
    "\n",
    "    end=time.time()\n",
    "    print (\"Total Time for epoch: %d: %lf\"%(epoch, float(end-start)))\n",
    "    print (\"Loss at epoch %d = %f\"%(epoch, average_loss))\n",
    "\n",
    "    if average_loss<min_loss:\n",
    "        min_loss=average_loss\n",
    "        #torch.save(best_model.state_dict(), \"best_model.m\")\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        no_up=0\n",
    "    else:\n",
    "        no_up+=1\n",
    "    if no_up==10:\n",
    "        break\n",
    "torch.save(best_model.state_dict(), \"data/graph_embd_yoga/output/des_net/b_des_net_model.m\")    \n",
    "NID.save_embeddings(best_model)\n",
    "\n",
    "best_model.eval()\n",
    "best_model.predict_utype()\n",
    "#best_model.predict_issue()\n",
    "best_model.eval()\n",
    "\n",
    "user2utype=best_model.predict_utype_all_users()\n",
    "#ad2issue=best_model.predict_issue_all_ad()\n",
    "des2utype=best_model.predict_utype_all_des()\n",
    "\n",
    "\n",
    "#output_dir='../../scratch/fb_ads_data/output/other/'\n",
    "with open('data/graph_embd_yoga/output/des_net/predicted_utypes_des_net.pickle', 'wb') as out_file:\n",
    "    pickle.dump([user2utype, des2utype], out_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load only best model (saved)\n",
    "NID=NodeInputData(batch_size=16)\n",
    "print (\"Creating batches...\")\n",
    "NID.get_nodes(NID.graph)\n",
    "print (\"Batches Created!\")\n",
    "\n",
    "print (\"Initializing the model...\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "#best_model = Embedder(cuda_available=True)\n",
    "best_model = Embedder(cuda_available=True)\n",
    "best_model.load_state_dict(torch.load(\"data/graph_embd_yoga/output/des_net/b_des_net_model.m\"))\n",
    "\n",
    "NID.save_embeddings(best_model)\n",
    "\n",
    "best_model.eval()\n",
    "best_model.predict_utype()\n",
    "best_model.eval()\n",
    "\n",
    "user2utype=best_model.predict_utype_all_users()\n",
    "#ad2issue=best_model.predict_issue_all_ad()\n",
    "des2utype=best_model.predict_utype_all_des()\n",
    "\n",
    "\n",
    "#output_dir='../../scratch/fb_ads_data/output/other/'\n",
    "with open('data/graph_embd_yoga/output/des_net/predicted_utypes_des_net.pickle', 'wb') as out_file:\n",
    "    pickle.dump([user2utype, des2utype], out_file)\n",
    "\n",
    "\n",
    "# des + net: accuracy: 0.7811704834605598, macro_f1 0.7533442802408319"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
